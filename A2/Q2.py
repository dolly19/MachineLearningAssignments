# -*- coding: utf-8 -*-
"""MLA2_Q2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZpDRuisfOVC2RHRQGx5dG4XE23Sfy1V
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

import warnings
warnings.filterwarnings('ignore')

#importing all needed libraries
from sklearn.utils import shuffle
from math import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import check_random_state
from sklearn.preprocessing import OneHotEncoder
import pickle
from sklearn.neural_network import MLPClassifier

"""#Question 2"""

# loading MNIST dataset
train_df = pd.read_csv('mnist_train.csv')
test_df = pd.read_csv('mnist_test.csv')

#pre-processing
frames = [train_df, test_df]
data = pd.concat(frames)
y= data['label']
X= data.drop(['label'],axis=1)
X = X.to_numpy()
y = y.to_numpy()

# shuffling dataset
random_state=check_random_state(0)
permutation=random_state.permutation(X.shape[0])
X=X[permutation]
y=y[permutation]
X=X.reshape((X.shape[0],-1))
print(X.shape), print(y.shape)

# splitting data into train test val : 7:2:1
m = X.shape[0]
train_size = int(m*0.70)
fold = int(((m-train_size)*2)/3)
test_size = m-train_size-fold
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[-test_size:], y[-test_size:]
X_val , y_val = X[train_size:-test_size], y[train_size:-test_size]
print(X_train.shape), print(y_train.shape)
print(X_val.shape), print(y_val.shape)
print(X_test.shape), print(y_test.shape)

# standardising and scaling data using StandardScaler
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
X_val=scaler.transform(X_val)

# hot encoding 
encoder=OneHotEncoder(sparse=False,categories='auto')
y_train = encoder.fit_transform(y_train.reshape(len(y_train),-1))
y_test = encoder.transform(y_test.reshape(len(y_test),-1))
y_val = encoder.transform(y_val.reshape(len(y_val),-1))

"""#Implementation of MyNeuralNetwork Classifier"""

class MyNeuralNetwork():
    
    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax','leakyRelu']
    weight_inits = ['zero', 'random', 'normal']

    def __init__(self, n_layers, layers_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):
        
        self.layers_sizes=layers_sizes[1:]
        self.activation=activation
        self.learning_rate=learning_rate
        self.weight_init=weight_init
        self.batch_size=batch_size
        self.num_epochs=num_epochs
        self.weights={}
        self.n_layers=len(self.layers_sizes)
        self.num_samples=0
        self.training_loss_values=[]
        self.validation_loss_values=[]
        
        if activation not in self.acti_fns:
            raise Exception('Incorrect Activation Function')

        if weight_init not in self.weight_inits:
            raise Exception('Incorrect Weight Initialization Function')
        pass

    #activation functions
    def relu(self, X):
        return np.maximum(0,X)

    def relu_grad(self, X):
        X[X<=0]=0
        X[X>0]=1
        return X

    def sigmoid(self, X):
        return 1.0/(1.0+np.exp(-X))

    def sigmoid_grad(self, X):
        var=self.sigmoid(X)
        return var*(1-var)

    def linear(self, X):
        return X

    def linear_grad(self, X):
        X[X<=0]=1
        X[X>0]=1
        return X

    def tanh(self, X):
        return np.tanh(X)
      
    def tanh_grad(self, X):
        return 1-np.power(self.tanh(X),2)

    def leakyRelu(self, X):
         return np.maximum(X, 0.01 * X)

    def leakyRelu_grad(self, X, out):
        return X * (np.where(out > 0, 1, 0))

    def softmax(self, X):
        expX=np.exp(X-np.max(X))
        return expX/expX.sum(axis=0,keepdims=True)

    def softmax_grad(self, X):
        s = X.reshape(-1,1)
        return np.diagflat(X)-np.dot(s,np.transpose(s))

    def zero_init(self, shape):
        return np.zeros((shape[0],shape[1]))

    def random_init(self, shape):
        return np.random.randn(shape[0],shape[1])*0.01

    def normal_init(self, shape):
        return np.random.normal(size=(shape[0],shape[1]))*0.01

    #intializing the weights
    def weight_initialization(self):
        np.random.seed(30)
        for i in range(1,len(self.layers_sizes)):
            r= self.layers_sizes[i]
            c=self.layers_sizes[i-1]
            if self.weight_init=="normal":
              self.weights["weight"+str(i)]=self.normal_init((r,c))
              self.weights["bias"+str(i)]=self.zero_init((r,1))

            elif self.weight_init=="random":
              self.weights["weight"+str(i)]=self.random_init((r,c))
              self.weights["bias"+str(i)]=self.zero_init((r,1))

            elif self.weight_init=="zero":
              self.weights["weight"+str(i)]=self.zero_init((r,c))
              self.weights["bias"+str(i)]=self.zero_init((r,1))


    def val_intializ(self , x, y):
      self.XVAL = x
      self.YVAL= y

    # intializing activation function based on activation parameter
    def activation_initialization(self,input):
      if self.activation=="sigmoid":
        out=self.sigmoid(input)
      elif self.activation=="linear":
        out=self.linear(input)
      elif self.activation=="relu":
        out=self.relu(input)
      elif self.activation=="softmax":
        out=self.softmax(input)
      elif self.activation=="tanh":
        out=self.tanh(input)
      elif self.activation=="leakyRelu":
        out=self.leakyRelu(input)
      return out

    # forward phase
    def forward(self,X):
      out=np.transpose(X)
      # for storing input, weights and outs of activation in each layer for each neuron 
      d_dictionary={} 
      
      for l in range(self.n_layers-1):
          # wx+b form
          input=self.weights["weight"+str(l+1)].dot(out)+self.weights["bias"+str(l+1)]
          
           #intializing the activation fucntion
          out = self.activation_initialization(input)

          d_dictionary["input"+str(l+1)]=input
          d_dictionary["weight"+str(l+1)]=self.weights["weight"+str(l+1)]
          d_dictionary["out"+str(l+1)]=out
      
      # out of layer becomes the input for next layer 
      input=self.weights["weight"+str(self.n_layers)].dot(out)
      input=input+self.weights["bias"+str(self.n_layers)]

      # applying softmax at final layer to calculate probabilities
      out=self.softmax(input)

      d_dictionary["input"+str(self.n_layers)]=input
      d_dictionary["out"+str(self.n_layers)]=out
      d_dictionary["weight"+str(self.n_layers)]=self.weights["weight"+str(self.n_layers)]

      return out,d_dictionary


    # backpropagation
    def backward(self, X, Y, d_dictionary):
        
        L = self.n_layers
        derivatives={}
        d_dictionary["out0"]=np.transpose(X)
        out=d_dictionary["out"+str(L)]

        #calculating derivative term at final layer
        dinput=out-np.transpose(Y)
        dbias=np.sum(dinput,axis=1,keepdims=True)/self.batch_size

        dweight=dinput.dot(np.transpose(d_dictionary["out"+str(L-1)]))
        dweight=dweight/self.batch_size

        weighted_input=np.transpose(d_dictionary["weight"+str(L)])
        weighted_input=weighted_input.dot(dinput)
        derivatives["dweight"+str(L)]=dweight
        derivatives["dbias"+str(L)]=dbias
 
        
        for i in range(L-1,0,-1):
            inp_index = "input"+str(i)
            out_index = "out"+str(i)
            # applying gradient of activation fn on the basis of activation value
            if self.activation=="sigmoid":
              grad=self.sigmoid_grad(d_dictionary[inp_index])
            elif self.activation=="relu":
              grad=self.relu_grad(d_dictionary[inp_index])
            elif self.activation=="leakyRelu":
              grad=self.leakyRelu_grad(d_dictionary[inp_index],d_dictionary[out_index])
            elif self.activation=="linear":
              grad=self.linear_grad(d_dictionary[inp_index])
            elif self.activation=="tanh":
              grad=self.tanh_grad(d_dictionary[inp_index])
            elif self.activation=="softmax":
              grad=self.softmax_grad(d_dictionary[inp_index])

            dinput=np.multiply(weighted_input,grad)
            dbias=np.sum(dinput,axis=1,keepdims=True)/self.batch_size
            value=np.transpose(d_dictionary["out"+str(i-1)])
            dweight=dinput.dot(value)
            dweight=dweight/self.batch_size
            
            if i>=2:
                weighted_input=np.transpose(d_dictionary["weight"+str(i)])
                weighted_input=weighted_input.dot(dinput)

            #storing change in weights
            derivatives["dbias"+str(i)]=dbias
            derivatives["dweight"+str(i)]=dweight

        return derivatives


    # weight update function to update the weights after a forward and backprop traversal
    def weight_update(self, derivatives):
      for j in range(1,self.n_layers+1):
        self.weights["weight"+str(j)]-=np.multiply(self.learning_rate,derivatives["dweight"+str(j)])
        self.weights["bias"+str(j)]-=np.multiply(self.learning_rate,derivatives["dbias"+str(j)])
        
    #fit function to fit the model
    def fit(self, X, Y ,x,y):
        np.random.seed(40)
        self.val_intializ(x,y)
        self.num_samples=X.shape[0]
        self.layers_sizes.insert(0,X.shape[1])
        #initlizing weights
        self.weight_initialization()
        epoch=self.num_epochs//5

        # iterating for each epochs
        for i in range(self.num_epochs):
            # creating batches of dataset of specified batch size
            X,Y=shuffle(X,Y,random_state=i)
            #define batch size
            num_batches=X.shape[0]//self.batch_size
            train_x=np.vsplit(X,num_batches)
            train_y=np.vsplit(Y,num_batches)
            train_cost=0
            
            # batch iteration and applying forward and backward propagation
            for i in range(num_batches):
              #forward propagation
              A,d_collection=self.forward(train_x[i])
              #calculating training cost using cross entropy loss for every batch
              loss = self.cross_entropy_loss(A,train_y[i])/num_batches
              train_cost = train_cost+loss
              derivatives=self.backward(train_x[i],train_y[i],d_collection)

              self.weight_update(derivatives)
              
            if i%epoch==0:
                print("Training accuracy score :",self.score(X,Y))
            
            # storing training and validation losses 
            
            self.training_loss_values.append(train_cost)
            prob = self.predict_proba(self.XVAL)
            val_cost = self.cross_entropy_loss(prob,self.YVAL)
            self.validation_loss_values.append(val_cost)
        return self

    # function for calculating loss
    def cross_entropy_loss(self, A, y):
      loss = (-np.mean(np.multiply(y,np.log(np.transpose(A)))))
      return loss
    
    #function for predicting probabilities at last layer after forward propagation
    def predict_proba(self, X):
        prob,cache=self.forward(X)
        return prob

    # function for calculating predicting values
    def predict(self, X):
        prob=self.predict_proba(X)
        y_pred=np.argmax(prob,axis=0)
        return y_pred
    
    # function for calculating accuracy
    def score(self, X, Y):
        y_pred=self.predict(X)
        Y=np.argmax(Y,axis=1)
        return (y_pred==Y).mean()

    # plot function for plotting training loss vs epoch and validation loss vs epoch
    def plot_cost(self):
        plt.figure()
        plt.plot(np.arange(len(self.training_loss_values)),self.training_loss_values, 'b-', label='Training Loss vs epoch')
        plt.plot(np.arange(len(self.validation_loss_values)),self.validation_loss_values, 'r-', label='Validation Loss vs epoch')
        plt.xlabel("Epochs")
        plt.ylabel("Loss Values")
        plt.legend()
        plt.show()

"""#Question 2.1 and Question 2.2"""

#for ReLU
nn=MyNeuralNetwork(6,[784,256,128,64,32,10],"relu",0.08,"normal",200,150)
nn.fit(X_train,y_train,X_val,y_val)
print('For relu activation function')
print("Training Accuracy:",nn.score(X_train,y_train))
print("Testing Accuracy:",nn.score(X_test,y_test))
nn.plot_cost()

#for tanh
nn=MyNeuralNetwork(6,[784,256,128,64,32,10],"tanh",0.08,"normal",200,150)
nn.fit(X_train,y_train,X_val,y_val)
print('For tanh activation function')
print("Training Accuracy:",nn.score(X_train,y_train))
print("Testing Accuracy:",nn.score(X_test,y_test))
nn.plot_cost()

#for linear
nn=MyNeuralNetwork(6,[784,256,128,64,32,10],"linear",0.08,"normal",200,150)
nn.fit(X_train,y_train,X_val,y_val)
print('For linear activation function')
print("Training Accuracy:",nn.score(X_train,y_train))
print("Testing Accuracy:",nn.score(X_test,y_test))
nn.plot_cost()

#for leaky Relu
nn=MyNeuralNetwork(6,[784,256,128,64,32,10],"leakyRelu",0.08,"normal",200,150)
nn.fit(X_train,y_train,X_val,y_val)
print('For leakyRelu activation function')
print("Training Accuracy:",nn.score(X_train,y_train))
print("Testing Accuracy:",nn.score(X_test,y_test))
nn.plot_cost()

#for sigmoid
nn=MyNeuralNetwork(6,[784,256,128,64,32,10],"sigmoid",0.08,"normal",50,150)
nn.fit(X_train,y_train,X_val,y_val)
print('For ReLU activation function')
print("Training Accuracy:",nn.score(X_train,y_train))
print("Testing Accuracy:",nn.score(X_test,y_test))
nn.plot_cost()

# f = open("/content/gdrive/MyDrive/ML datasets/weights_sigmoid.pkl", "wb")
# pickle.dump(nn.weights,f)
# f.close()

"""#Question 2.4"""

#MLP Classifier using sigmoid
clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="logistic", batch_size=50, learning_rate_init=0.08, max_iter=150, random_state=123, solver="sgd")
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

#MLP Classifier using linear
clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="identity", batch_size=200, learning_rate_init=0.08, max_iter=150, random_state=123,, solver="sgd")
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

#MLP Classifier using tanh
clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", batch_size=200, learning_rate_init=0.08, max_iter=150, random_state=123, solver="sgd")
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

#MLP Classifier using ReLU
clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="relu", batch_size=200, learning_rate_init=0.08, max_iter=150, random_state=123, solver="sgd")
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

"""#Question 2.5"""

learning_rate = [0.001,0.01,0.1,1]

#training the model for each learning rate
for x in learning_rate:
  print("for learning rate :",x)
  nn = MyNeuralNetwork(6,[784,256,128,64,32,10],"tanh",x,"normal",200,100)
  nn.fit(X_train,y_train,X_val,y_val)
  print("Training Accuracy:",nn.score(X_train,y_train))
  print("Testing Accuracy:",nn.score(X_test,y_test))
  nn.plot_cost()