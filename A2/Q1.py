# -*- coding: utf-8 -*-
"""MLA2_Q1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1agI79vmmYmIqlDzvs37jXqIHEbbLfBru
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

import warnings
warnings.filterwarnings('ignore')

#importing all needed libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from random import seed
from random import randrange
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.base import clone
import random

"""#Question1"""

#loading the dataset
data = pd.read_csv("PRSA_data_2010.1.1-2014.12.31.csv")

print(data.isnull().sum())
print(data.shape)
print(data.cbwd.unique())
print(data.month.unique())

"""preproccessing : 
1)dropping 'No' column
2)converting categorical value to integer
3)replacing null values to mean of the column 
"""
data = data.drop(['No'],axis=1)
data[['cbwd']]=data[['cbwd']].apply(lambda col:pd.Categorical(col).codes)
mean_pm = data['pm2.5'].mean()
data['pm2.5'].fillna(mean_pm, inplace=True)

#shuffling of data to ensure iid 
np.random.seed(0)
data = data.sample(frac=1)
y= data['month']
X= data.drop(['month'],axis=1)

#spilting the data into train val test : 70:15:15
m = X.shape[0]
train_size = int(m*0.70)
fold = int(((m-train_size)*1.5)/3)
test_size = m-train_size-fold
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[-test_size:], y[-test_size:]
X_val , y_val = X[train_size:-test_size], y[train_size:-test_size]

print(X_train.shape), print(y_train.shape)
print(X_val.shape), print(y_val.shape)
print(X_test.shape), print(y_test.shape)

"""# Part A"""

def accuracy_score(y_actual,y_predict):
  return (y_predict == y_actual).astype(int).mean()

#training for criterion ='gini'
model_gini = DecisionTreeClassifier(criterion='gini',random_state=123)
model_gini.fit(X_train, y_train)
y_predict = model_gini.predict(X_test)
acc = accuracy_score(y_test, y_predict)
print("Accuracy using gini : ", acc)

#training for criterion ='entropy'
model_entropy = DecisionTreeClassifier(criterion='entropy', random_state=123)
model_entropy.fit(X_train, y_train)
y_predict = model_entropy.predict(X_test)
acc=accuracy_score(y_test, y_predict)
print("Accuracy using entropy : ",acc)

# print(classification_report(y_test, y_pred, labels=[1,2,3,4,5,6,7,8,9,10,11,12]),'\n')

"""#Part B"""

depths = [2,3,4,8,10,15,30]
accuracies_testing = list() 
accuracies_training = list() 

#iterating over different depths and storing the accuracies of training and testing
for x in depths:
   model_dt = DecisionTreeClassifier(criterion='entropy', max_depth=x, random_state=123)
   model_dt.fit(X_train, y_train)
   y_predict_train = model_dt.predict(X_train)
   y_predict_test = model_dt.predict(X_test)
   acc_train=accuracy_score(y_train, y_predict_train)
   acc_test=accuracy_score(y_test, y_predict_test)
   accuracies_testing.append(acc_train)
   accuracies_training.append(acc_test)

#plotting the graph
plt.plot(depths, accuracies_training, marker='o', color='red', label='training')
plt.plot(depths, accuracies_testing, marker='o', color='blue', label='testing')
plt.xlabel('Depth values')
plt.ylabel('Accuracy')
plt.legend()
plt.title("Accuracy vs Depth Graph")

"""#Part C"""

dataset = X_train.copy(deep=True)
dataset['month'] = y_train

#function to extract 50% of the training data
def spilting(df):
  df = df.sample(frac=1)
  y= df['month']
  X= df.drop(['month'],axis=1)
  m = X.shape[0]
  spilt_size = int(0.5 * m)
  X_train = X[:spilt_size]
  Y_train = y[:spilt_size]
  return X_train, Y_train

#ensembling function to train the model
def ensembling(depth=3 , trees =100):
  prediction = list()
  for x in range(0,trees):
    X , y = spilting(dataset)
    model_dt = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=123)
    model_dt.fit(X, y)
    y_predict = model_dt.predict(X_test)
    prediction.append(y_predict)
  return np.array(prediction)

#function to predict lables by taking majority vote
def max_Vote(prediction):
  final_prediction  = list()
  for x in range(0,len(prediction[0])):
    predict = prediction[:,x]
    val = np.bincount(predict).argmax()
    final_prediction.append(val)
  # print(np.array(final_prediction))
  # print(len(final_prediction))
  return final_prediction

prediction =  ensembling()
ensemble_pred = max_Vote(prediction)
print("Accuracy using Ensembling : ",accuracy_score(y_test,ensemble_pred))

"""#Part D"""

Y_train = y_train[24102:]

#training the model for different depths
depths = [4,8,10,15,20,30]

for x in depths: 
  print("For depth ",x)
  prediction =  ensembling(x)
  ensemble_pred = max_Vote(prediction)
  print("Accuracy on train set : ",accuracy_score(Y_train,ensemble_pred))
  print("Accuracy on val set : ",accuracy_score(y_val,ensemble_pred))
  print("Accuracy on test set : ",accuracy_score(y_test,ensemble_pred))

#training the model for number of tress
trees = [50,100,150]

for x in trees:
  print("For number of trees ",x)
  prediction =  ensembling(30,x)
  ensemble_pred = max_Vote(prediction)
  print("Accuracy on train set : ",accuracy_score(Y_train,ensemble_pred))
  print("Accuracy on val set : ",accuracy_score(y_val,ensemble_pred))
  print("Accuracy on test set : ",accuracy_score(y_test,ensemble_pred))

"""#Part E"""

#Adaboost
estimators = [4,8,10,15,20]
for x in estimators:
  print("for estimator :",x)
  DT= DecisionTreeClassifier(criterion='entropy',  random_state=123)
  ada_ensemble = AdaBoostClassifier(base_estimator = DT, n_estimators= x, random_state=123)
  ada_ensemble.fit(X_train, y_train)
  y_pred = ada_ensemble.predict(X_test)
  print("Accuracy on test set : ",accuracy_score(y_test,y_pred))